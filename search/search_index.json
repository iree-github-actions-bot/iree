{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IREE \u00b6 IREE ( I ntermediate R epresentation E xecution E nvironment 1 ) is an MLIR -based end-to-end compiler and runtime that lowers Machine Learning (ML) models to a unified IR that scales up to meet the needs of the datacenter and down to satisfy the constraints and special considerations of mobile and edge deployments. Key features \u00b6 Ahead-of-time compilation of scheduling and execution logic together Support for dynamic shapes, flow control, streaming, and other advanced model features Optimized for many CPU and GPU architectures Low overhead, pipelined execution for efficient power and resource usage Binary size as low as 30KB on embedded systems Debugging and profiling support Support matrix \u00b6 IREE supports importing from a variety of ML frameworks: TensorFlow TensorFlow Lite JAX PyTorch (planned) ONNX (hoped for) The IREE compiler tools run on Linux, Windows, and macOS and can generate efficient code for a variety of runtime platforms: Linux Windows Android macOS (planned) iOS (planned) Bare metal WebAssembly (planned) and architectures: ARM x86 RISC-V Support for hardware accelerators and APIs is also included: Vulkan CUDA Metal (planned) WebGPU (planned) Project architecture \u00b6 IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan , and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V . Workflow overview \u00b6 Using IREE involves these general steps: Import your model Work in your framework of choice, then run your model through one of IREE's import tools. Select your deployment configuration Identify your target platform, accelerator(s), and other constraints. Compile your model Compile through IREE, picking compilation targets based on your deployment configuration. Run your model Use IREE's runtime components to execute your compiled model. Importing models from ML frameworks \u00b6 IREE supports importing models from a growing list of ML frameworks and model formats: TensorFlow TensorFlow Lite JAX Selecting deployment configurations \u00b6 IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators. What platforms are you targeting? Desktop? Mobile? An embedded system? What hardware should the bulk of your model run on? CPU? GPU? How fixed is your model itself? Can the weights be changed? Do you want to support loading different model architectures dynamically? IREE supports the full set of these configurations using the same underlying technology. Compiling models \u00b6 Model compilation is performed ahead-of-time on a host machine for any combination of targets . The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic. For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution , native code with static or dynamic linkage and the associated function calls are generated. Running models \u00b6 IREE offers a low level C API, as well as several specialized sets of bindings for running IREE models using other languages: C API Python TensorFlow Lite Communication channels \u00b6 GitHub issues : Feature requests, bugs, and other work tracking IREE Discord server : Daily development discussions with the core team and collaborators iree-discuss email list : Announcements, general and low-priority discussion Roadmap \u00b6 IREE is in the early stages of development and is not yet ready for broad adoption. Check out the long-term design roadmap to get a sense of where we're headed. We plan on a quarterly basis using OKRs . Review our latest objectives to see what we're up to. We use GitHub Projects to track progress on IREE components and specific efforts and GitHub Milestones to track the work associated with plans for each quarter. Pronounced \"eerie\" and often styled with the emoji \u21a9","title":"Home"},{"location":"#iree","text":"IREE ( I ntermediate R epresentation E xecution E nvironment 1 ) is an MLIR -based end-to-end compiler and runtime that lowers Machine Learning (ML) models to a unified IR that scales up to meet the needs of the datacenter and down to satisfy the constraints and special considerations of mobile and edge deployments.","title":"IREE"},{"location":"#key-features","text":"Ahead-of-time compilation of scheduling and execution logic together Support for dynamic shapes, flow control, streaming, and other advanced model features Optimized for many CPU and GPU architectures Low overhead, pipelined execution for efficient power and resource usage Binary size as low as 30KB on embedded systems Debugging and profiling support","title":"Key features"},{"location":"#support-matrix","text":"IREE supports importing from a variety of ML frameworks: TensorFlow TensorFlow Lite JAX PyTorch (planned) ONNX (hoped for) The IREE compiler tools run on Linux, Windows, and macOS and can generate efficient code for a variety of runtime platforms: Linux Windows Android macOS (planned) iOS (planned) Bare metal WebAssembly (planned) and architectures: ARM x86 RISC-V Support for hardware accelerators and APIs is also included: Vulkan CUDA Metal (planned) WebGPU (planned)","title":"Support matrix"},{"location":"#project-architecture","text":"IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan , and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V .","title":"Project architecture"},{"location":"#workflow-overview","text":"Using IREE involves these general steps: Import your model Work in your framework of choice, then run your model through one of IREE's import tools. Select your deployment configuration Identify your target platform, accelerator(s), and other constraints. Compile your model Compile through IREE, picking compilation targets based on your deployment configuration. Run your model Use IREE's runtime components to execute your compiled model.","title":"Workflow overview"},{"location":"#importing-models-from-ml-frameworks","text":"IREE supports importing models from a growing list of ML frameworks and model formats: TensorFlow TensorFlow Lite JAX","title":"Importing models from ML frameworks"},{"location":"#selecting-deployment-configurations","text":"IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators. What platforms are you targeting? Desktop? Mobile? An embedded system? What hardware should the bulk of your model run on? CPU? GPU? How fixed is your model itself? Can the weights be changed? Do you want to support loading different model architectures dynamically? IREE supports the full set of these configurations using the same underlying technology.","title":"Selecting deployment configurations"},{"location":"#compiling-models","text":"Model compilation is performed ahead-of-time on a host machine for any combination of targets . The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic. For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution , native code with static or dynamic linkage and the associated function calls are generated.","title":"Compiling models"},{"location":"#running-models","text":"IREE offers a low level C API, as well as several specialized sets of bindings for running IREE models using other languages: C API Python TensorFlow Lite","title":"Running models"},{"location":"#communication-channels","text":"GitHub issues : Feature requests, bugs, and other work tracking IREE Discord server : Daily development discussions with the core team and collaborators iree-discuss email list : Announcements, general and low-priority discussion","title":"Communication channels"},{"location":"#roadmap","text":"IREE is in the early stages of development and is not yet ready for broad adoption. Check out the long-term design roadmap to get a sense of where we're headed. We plan on a quarterly basis using OKRs . Review our latest objectives to see what we're up to. We use GitHub Projects to track progress on IREE components and specific efforts and GitHub Milestones to track the work associated with plans for each quarter. Pronounced \"eerie\" and often styled with the emoji \u21a9","title":"Roadmap"},{"location":"bindings/c-api/","text":"C API bindings \u00b6 IREE provides a low level C API for its runtime 1 , which can be used directly or through higher level APIs and language bindings built on top of it. API header files are organized by runtime component: Component header file Overview iree/base/api.h Core API, type definitions, ownership policies, utilities iree/vm/api.h VM APIs: loading modules, I/O, calling functions iree/hal/api.h HAL APIs: device management, synchronization, accessing hardware features The iree/samples/ directory demonstrates several ways to use IREE's C API. Prerequisites \u00b6 To use IREE's C API, you will need to build the runtime from source . The iree-template-cpp community project also shows how to integrate IREE into an external project using CMake 2 . Concepts \u00b6 By default, IREE uses its own tiny Virtual Machine (VM) at runtime to interpret program instructions on the host system. VM instructions may also be lowered further to LLVM IR, C, or other representations for static or resource constrained deployment. The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. It builds streams of more complex program logic and dense math into command buffers that are dispatched to hardware backends through the Hardware Abstraction Layer (HAL) interface. Most interaction with IREE's C API involves either the VM or the HAL. IREE VM \u00b6 VM instances can serve multiple isolated execution contexts VM contexts are effectively sandboxes for loading modules and running programs VM modules provide extra functionality to execution contexts , such as access to hardware accelerators through the HAL. Compiled user programs are also modules. IREE HAL \u00b6 HAL drivers are used to enumerate and create HAL devices HAL devices interface with hardware, such as by allocating device memory, preparing executables, recording and dispatching command buffers, and synchronizing with the host HAL buffers and buffer views represent storage and shaped/typed views into that storage (aka \"tensors\") Using the C API \u00b6 Setup \u00b6 Include headers: #include \"iree/base/api.h\" #include \"iree/hal/api.h\" #include \"iree/vm/api.h\" // The VM bytecode and HAL modules will typically be included, along // with those for the specific HAL drivers your application uses. // Functionality extensions can be used via custom modules. #include \"iree/modules/hal/module.h\" #include \"iree/hal/dylib/registration/driver_module.h\" #include \"iree/vm/bytecode_module.h\" Check the API version and register components: // Statically linking the runtime should never have version conflicts, // however dynamically linking against a runtime shared object must // always verify that the version is as expected. iree_api_version_t actual_version ; IREE_CHECK_OK ( iree_api_version_check ( IREE_API_VERSION_LATEST , & actual_version )); // Modules with custom types must be statically registered before use. IREE_CHECK_OK ( iree_hal_module_register_types ()); // Device drivers are managed through registries. // Applications may use multiple registries to more finely control driver // lifetimes and visibility. IREE_CHECK_OK ( iree_hal_dylib_driver_module_register ( iree_hal_driver_registry_default ())); Tip The IREE_CHECK_OK() macro calls assert() if an error occurs. Applications should propagate errors and handle or report them as desired. Configure stateful objects \u00b6 Create a VM instance along with a HAL driver and device: // Applications should try to reuse instances so resource usage across contexts // is handled and extraneous device interaction is avoided. iree_vm_instance_t * instance = NULL ; IREE_CHECK_OK ( iree_vm_instance_create ( iree_allocator_system (), & instance )); // We use the CPU \"dylib\" driver in this example, but could use a different // driver like the GPU \"vulkan\" driver. The driver(s) used should match with // the target(s) specified during compilation. iree_hal_driver_t * driver = NULL ; IREE_CHECK_OK ( iree_hal_driver_registry_try_create_by_name ( iree_hal_driver_registry_default (), iree_string_view_literal ( \"dylib\" ), iree_allocator_system (), & driver )); // Drivers may support multiple devices, such as when a machine has multiple // GPUs. You may either enumerate devices and select based on their properties, // or just use the default device. iree_hal_device_t * device = NULL ; IREE_CHECK_OK ( iree_hal_driver_create_default_device ( driver , iree_allocator_system (), & device )); // Create a HAL module initialized to use the newly created device. // We'll load this module into a VM context later. iree_vm_module_t * hal_module = NULL ; IREE_CHECK_OK ( iree_hal_module_create ( device , iree_allocator_system (), & hal_module )); // The reference to the driver can be released now. iree_hal_driver_release ( driver ); Tip The default iree_allocator_system() is backed by malloc and free , but custom allocators may also be used. Load a vmfb bytecode module containing program data: // (Application-specific loading into memory, such as loading from a file) iree_vm_module_t * bytecode_module = NULL ; IREE_CHECK_OK ( iree_vm_bytecode_module_create ( iree_const_byte_span_t { module_data , module_size }, /*flatbuffer_allocator=*/ iree_allocator_null (), /*allocator=*/ iree_allocator_system (), & bytecode_module )); Note Many IREE samples use c_embed_data to embed vmfb files as C code to avoid file I/O and ease portability. Applications should use what makes sense for their platforms and deployment configurations. Create a VM context and load modules into it: iree_vm_context_t * context = NULL ; iree_vm_module_t * modules [ 2 ] = { hal_module , bytecode_module }; IREE_CHECK_OK ( iree_vm_context_create_with_modules ( instance , IREE_VM_CONTEXT_FLAG_NONE , modules , IREE_ARRAYSIZE ( modules ), iree_allocator_system (), & context )); // References to the modules can be released now. iree_vm_module_release ( hal_module ); iree_vm_module_release ( bytecode_module ); Look up the function(s) to call: iree_vm_function_t main_function ; IREE_CHECK_OK ( iree_vm_context_resolve_function ( context , iree_string_view_literal ( \"module.main_function\" ), & main_function )); Invoke functions \u00b6 // (Application-specific I/O buffer setup, making data available to the device) IREE_CHECK_OK ( iree_vm_invoke ( context , main_function , /*policy=*/ NULL , inputs , outputs , iree_allocator_system ())); // (Application-specific output buffer retrieval and reading back from the device) Cleanup resources \u00b6 iree_hal_device_release ( device ); iree_vm_context_release ( context ); iree_vm_instance_release ( instance ); We are exploring adding a C API for IREE's compiler, see this GitHub issue \u21a9 We plan on deploying via vcpkg in the future too, see this GitHub project \u21a9","title":"C API"},{"location":"bindings/c-api/#c-api-bindings","text":"IREE provides a low level C API for its runtime 1 , which can be used directly or through higher level APIs and language bindings built on top of it. API header files are organized by runtime component: Component header file Overview iree/base/api.h Core API, type definitions, ownership policies, utilities iree/vm/api.h VM APIs: loading modules, I/O, calling functions iree/hal/api.h HAL APIs: device management, synchronization, accessing hardware features The iree/samples/ directory demonstrates several ways to use IREE's C API.","title":"C API bindings"},{"location":"bindings/c-api/#prerequisites","text":"To use IREE's C API, you will need to build the runtime from source . The iree-template-cpp community project also shows how to integrate IREE into an external project using CMake 2 .","title":"Prerequisites"},{"location":"bindings/c-api/#concepts","text":"By default, IREE uses its own tiny Virtual Machine (VM) at runtime to interpret program instructions on the host system. VM instructions may also be lowered further to LLVM IR, C, or other representations for static or resource constrained deployment. The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. It builds streams of more complex program logic and dense math into command buffers that are dispatched to hardware backends through the Hardware Abstraction Layer (HAL) interface. Most interaction with IREE's C API involves either the VM or the HAL.","title":"Concepts"},{"location":"bindings/c-api/#iree-vm","text":"VM instances can serve multiple isolated execution contexts VM contexts are effectively sandboxes for loading modules and running programs VM modules provide extra functionality to execution contexts , such as access to hardware accelerators through the HAL. Compiled user programs are also modules.","title":"IREE VM"},{"location":"bindings/c-api/#iree-hal","text":"HAL drivers are used to enumerate and create HAL devices HAL devices interface with hardware, such as by allocating device memory, preparing executables, recording and dispatching command buffers, and synchronizing with the host HAL buffers and buffer views represent storage and shaped/typed views into that storage (aka \"tensors\")","title":"IREE HAL"},{"location":"bindings/c-api/#using-the-c-api","text":"","title":"Using the C API"},{"location":"bindings/c-api/#setup","text":"Include headers: #include \"iree/base/api.h\" #include \"iree/hal/api.h\" #include \"iree/vm/api.h\" // The VM bytecode and HAL modules will typically be included, along // with those for the specific HAL drivers your application uses. // Functionality extensions can be used via custom modules. #include \"iree/modules/hal/module.h\" #include \"iree/hal/dylib/registration/driver_module.h\" #include \"iree/vm/bytecode_module.h\" Check the API version and register components: // Statically linking the runtime should never have version conflicts, // however dynamically linking against a runtime shared object must // always verify that the version is as expected. iree_api_version_t actual_version ; IREE_CHECK_OK ( iree_api_version_check ( IREE_API_VERSION_LATEST , & actual_version )); // Modules with custom types must be statically registered before use. IREE_CHECK_OK ( iree_hal_module_register_types ()); // Device drivers are managed through registries. // Applications may use multiple registries to more finely control driver // lifetimes and visibility. IREE_CHECK_OK ( iree_hal_dylib_driver_module_register ( iree_hal_driver_registry_default ())); Tip The IREE_CHECK_OK() macro calls assert() if an error occurs. Applications should propagate errors and handle or report them as desired.","title":"Setup"},{"location":"bindings/c-api/#configure-stateful-objects","text":"Create a VM instance along with a HAL driver and device: // Applications should try to reuse instances so resource usage across contexts // is handled and extraneous device interaction is avoided. iree_vm_instance_t * instance = NULL ; IREE_CHECK_OK ( iree_vm_instance_create ( iree_allocator_system (), & instance )); // We use the CPU \"dylib\" driver in this example, but could use a different // driver like the GPU \"vulkan\" driver. The driver(s) used should match with // the target(s) specified during compilation. iree_hal_driver_t * driver = NULL ; IREE_CHECK_OK ( iree_hal_driver_registry_try_create_by_name ( iree_hal_driver_registry_default (), iree_string_view_literal ( \"dylib\" ), iree_allocator_system (), & driver )); // Drivers may support multiple devices, such as when a machine has multiple // GPUs. You may either enumerate devices and select based on their properties, // or just use the default device. iree_hal_device_t * device = NULL ; IREE_CHECK_OK ( iree_hal_driver_create_default_device ( driver , iree_allocator_system (), & device )); // Create a HAL module initialized to use the newly created device. // We'll load this module into a VM context later. iree_vm_module_t * hal_module = NULL ; IREE_CHECK_OK ( iree_hal_module_create ( device , iree_allocator_system (), & hal_module )); // The reference to the driver can be released now. iree_hal_driver_release ( driver ); Tip The default iree_allocator_system() is backed by malloc and free , but custom allocators may also be used. Load a vmfb bytecode module containing program data: // (Application-specific loading into memory, such as loading from a file) iree_vm_module_t * bytecode_module = NULL ; IREE_CHECK_OK ( iree_vm_bytecode_module_create ( iree_const_byte_span_t { module_data , module_size }, /*flatbuffer_allocator=*/ iree_allocator_null (), /*allocator=*/ iree_allocator_system (), & bytecode_module )); Note Many IREE samples use c_embed_data to embed vmfb files as C code to avoid file I/O and ease portability. Applications should use what makes sense for their platforms and deployment configurations. Create a VM context and load modules into it: iree_vm_context_t * context = NULL ; iree_vm_module_t * modules [ 2 ] = { hal_module , bytecode_module }; IREE_CHECK_OK ( iree_vm_context_create_with_modules ( instance , IREE_VM_CONTEXT_FLAG_NONE , modules , IREE_ARRAYSIZE ( modules ), iree_allocator_system (), & context )); // References to the modules can be released now. iree_vm_module_release ( hal_module ); iree_vm_module_release ( bytecode_module ); Look up the function(s) to call: iree_vm_function_t main_function ; IREE_CHECK_OK ( iree_vm_context_resolve_function ( context , iree_string_view_literal ( \"module.main_function\" ), & main_function ));","title":"Configure stateful objects"},{"location":"bindings/c-api/#invoke-functions","text":"// (Application-specific I/O buffer setup, making data available to the device) IREE_CHECK_OK ( iree_vm_invoke ( context , main_function , /*policy=*/ NULL , inputs , outputs , iree_allocator_system ())); // (Application-specific output buffer retrieval and reading back from the device)","title":"Invoke functions"},{"location":"bindings/c-api/#cleanup-resources","text":"iree_hal_device_release ( device ); iree_vm_context_release ( context ); iree_vm_instance_release ( instance ); We are exploring adding a C API for IREE's compiler, see this GitHub issue \u21a9 We plan on deploying via vcpkg in the future too, see this GitHub project \u21a9","title":"Cleanup resources"},{"location":"bindings/python/","text":"Python bindings \u00b6 IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler-snapshot IREE's generic compiler tools and helpers iree-runtime-snapshot IREE's runtime, including CPU and GPU backends iree-tools-tf-snapshot Tools for importing from TensorFlow iree-tools-tflite-snapshot Tools for importing from TensorFlow Lite iree-tools-xla-snapshot Tools for importing from XLA iree-jax-snapshot Tools for importing from JAX Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends. Warning The TensorFlow, TensorFlow Lite, and XLA packages are currently only available on Linux and macOS. They are not available on Windows yet (see this issue ). Prerequisites \u00b6 To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux and MacOS python -m venv .venv source .venv/bin/activate Windows python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py Installing IREE packages \u00b6 Prebuilt packages \u00b6 For now, packages can be installed from our GitHub releases : Minimal To install just the core IREE packages: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ --find-links https://github.com/google/iree/releases All packages To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ iree-tools-tflite-snapshot \\ iree-tools-xla-snapshot \\ --find-links https://github.com/google/iree/releases Info We plan to publish packages on PyPI as they become more stable. Building from source \u00b6 See Building Python bindings page for instructions for building from source. Using the Python bindings \u00b6 Troubleshooting \u00b6","title":"Python"},{"location":"bindings/python/#python-bindings","text":"IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler-snapshot IREE's generic compiler tools and helpers iree-runtime-snapshot IREE's runtime, including CPU and GPU backends iree-tools-tf-snapshot Tools for importing from TensorFlow iree-tools-tflite-snapshot Tools for importing from TensorFlow Lite iree-tools-xla-snapshot Tools for importing from XLA iree-jax-snapshot Tools for importing from JAX Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends. Warning The TensorFlow, TensorFlow Lite, and XLA packages are currently only available on Linux and macOS. They are not available on Windows yet (see this issue ).","title":"Python bindings"},{"location":"bindings/python/#prerequisites","text":"To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux and MacOS python -m venv .venv source .venv/bin/activate Windows python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py","title":"Prerequisites"},{"location":"bindings/python/#installing-iree-packages","text":"","title":"Installing IREE packages"},{"location":"bindings/python/#prebuilt-packages","text":"For now, packages can be installed from our GitHub releases : Minimal To install just the core IREE packages: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ --find-links https://github.com/google/iree/releases All packages To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ iree-tools-tflite-snapshot \\ iree-tools-xla-snapshot \\ --find-links https://github.com/google/iree/releases Info We plan to publish packages on PyPI as they become more stable.","title":"Prebuilt packages"},{"location":"bindings/python/#building-from-source","text":"See Building Python bindings page for instructions for building from source.","title":"Building from source"},{"location":"bindings/python/#using-the-python-bindings","text":"","title":"Using the Python bindings"},{"location":"bindings/python/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"bindings/tensorflow-lite/","text":"TensorFlow Lite bindings \u00b6 Todo Issue#5462 : write this documentation","title":"TensorFlow Lite"},{"location":"bindings/tensorflow-lite/#tensorflow-lite-bindings","text":"Todo Issue#5462 : write this documentation","title":"TensorFlow Lite bindings"},{"location":"blog/2021-07-19-tflite-tosa/","text":"Monday, July 19, 2021 By Rob Suderman and Jenni Kilduff TFLite Support via TOSA \u00b6 IREE can now execute TensorFlow Lite (TFLite) models through the use of TOSA , an open standard of common tensor operations, and a part of MLIR core. TOSA\u2019s high-level representation of tensor operations provides a common front-end for ingesting models from different frameworks. In this case we ingest a TFLite flatbuffer and compile it to TOSA IR, which IREE takes as an input format to compile to its various backends. Using TFLite as a frontend for IREE provides an alternative ingestion method for already existing models that could benefit from IREE\u2019s design. This enables models already designed for on-device inference to have an alternative path for execution without requiring any additional porting, while benefiting from IREE\u2019s improvements in buffer management, work dispatch system, and compact binary format. With continued improvements to IREE/MLIR\u2019s compilation performance, more optimized versions can be compiled and distributed to target devices without an update to the clientside environment. Today, we have validated floating point support for a variety of models, including mobilenet (v1, v2, and v3) and mobilebert . More work is in progress to support fully quantized models, and TFLite\u2019s hybrid quantization, along with dynamic shape support. Examples \u00b6 TFLite with IREE is available in Python and Java. We have a colab notebook that shows how to use IREE\u2019s python bindings and TFLite compiler tools to compile a pre-trained TFLite model from a flatbuffer and run using IREE. We also have an Android Java app that was forked from an existing TFLite demo app, swapping out the TFLite library for our own AAR. More information on IREE\u2019s TFLite frontend is available here .","title":"TFLite Support via TOSA"},{"location":"blog/2021-07-19-tflite-tosa/#tflite-support-via-tosa","text":"IREE can now execute TensorFlow Lite (TFLite) models through the use of TOSA , an open standard of common tensor operations, and a part of MLIR core. TOSA\u2019s high-level representation of tensor operations provides a common front-end for ingesting models from different frameworks. In this case we ingest a TFLite flatbuffer and compile it to TOSA IR, which IREE takes as an input format to compile to its various backends. Using TFLite as a frontend for IREE provides an alternative ingestion method for already existing models that could benefit from IREE\u2019s design. This enables models already designed for on-device inference to have an alternative path for execution without requiring any additional porting, while benefiting from IREE\u2019s improvements in buffer management, work dispatch system, and compact binary format. With continued improvements to IREE/MLIR\u2019s compilation performance, more optimized versions can be compiled and distributed to target devices without an update to the clientside environment. Today, we have validated floating point support for a variety of models, including mobilenet (v1, v2, and v3) and mobilebert . More work is in progress to support fully quantized models, and TFLite\u2019s hybrid quantization, along with dynamic shape support.","title":"TFLite Support via TOSA"},{"location":"blog/2021-07-19-tflite-tosa/#examples","text":"TFLite with IREE is available in Python and Java. We have a colab notebook that shows how to use IREE\u2019s python bindings and TFLite compiler tools to compile a pre-trained TFLite model from a flatbuffer and run using IREE. We also have an Android Java app that was forked from an existing TFLite demo app, swapping out the TFLite library for our own AAR. More information on IREE\u2019s TFLite frontend is available here .","title":"Examples"},{"location":"building-from-source/","text":"Building IREE from source \u00b6 Under construction.","title":"Building IREE from source"},{"location":"building-from-source/#building-iree-from-source","text":"Under construction.","title":"Building IREE from source"},{"location":"building-from-source/android/","text":"Android cross-compilation \u00b6 Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK Prerequisites \u00b6 Host environment setup \u00b6 You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps. Install Android NDK and ADB \u00b6 The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide . Configure and build \u00b6 Host configuration \u00b6 Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install Target configuration \u00b6 Build the runtime using the Android NDK toolchain: Linux and MacOS cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Windows cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device. Running Android tests \u00b6 Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : cd ../iree-build-android/ ctest --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine. Running tools directly \u00b6 Invoke the host compiler tools to produce a bytecode module flatbuffer: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmvx \\ iree/samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Push the Android runtime tools to the device, along with any flatbuffer files: adb push ../iree-build-android/iree/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/simple_abs_vmvx.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module -driver = vmvx \\ -module_file = /data/local/tmp/simple_abs_vmvx.vmfb \\ -entry_function = abs \\ -function_input = \"f32=-5\"","title":"Android cross-compilation"},{"location":"building-from-source/android/#android-cross-compilation","text":"Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK","title":"Android cross-compilation"},{"location":"building-from-source/android/#prerequisites","text":"","title":"Prerequisites"},{"location":"building-from-source/android/#host-environment-setup","text":"You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.","title":"Host environment setup"},{"location":"building-from-source/android/#install-android-ndk-and-adb","text":"The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide .","title":"Install Android NDK and ADB"},{"location":"building-from-source/android/#configure-and-build","text":"","title":"Configure and build"},{"location":"building-from-source/android/#host-configuration","text":"Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install","title":"Host configuration"},{"location":"building-from-source/android/#target-configuration","text":"Build the runtime using the Android NDK toolchain: Linux and MacOS cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Windows cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device.","title":"Target configuration"},{"location":"building-from-source/android/#running-android-tests","text":"Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : cd ../iree-build-android/ ctest --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine.","title":"Running Android tests"},{"location":"building-from-source/android/#running-tools-directly","text":"Invoke the host compiler tools to produce a bytecode module flatbuffer: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmvx \\ iree/samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Push the Android runtime tools to the device, along with any flatbuffer files: adb push ../iree-build-android/iree/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/simple_abs_vmvx.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module -driver = vmvx \\ -module_file = /data/local/tmp/simple_abs_vmvx.vmfb \\ -entry_function = abs \\ -function_input = \"f32=-5\"","title":"Running tools directly"},{"location":"building-from-source/getting-started/","text":"Getting started \u00b6 Prerequisites \u00b6 You will need to install CMake , the Ninja CMake generator, and the clang or MSVC C/C++ compilers: Note You are welcome to try different CMake generators and compilers, but IREE devs and CIs exclusively use these and other configurations are \"best effort\". Additionally, compilation on macOS is \"best effort\" as well, though we generally expect it to work due to its similarity with Linux. Patches to improve support for these are always welcome. Linux and macOS Install a compiler/linker (typically \"clang\" and \"lld\" package). Install CMake (typically \"cmake\" package). Install Ninja (typically \"ninja-build\" package). On a relatively recent Debian/Ubuntu: sudo apt install cmake ninja-build clang lld Windows Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Install CMake from the official downloads page Install Ninja either from the official site . Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details. Clone and build \u00b6 Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/google/iree.git cd iree git submodule update --init Configure then build all targets using CMake: Configure CMake: Linux and MacOS # Recommended for simple development using clang and lld: cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON \\ -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ \\ -DIREE_ENABLE_LLD = ON # Alternately, with system compiler and your choice of CMake generator: # cmake -B ../iree-build/ -S . # Additional quality of life CMake flags: # Enable ccache: # -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache Windows cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON Build: cmake --build ../iree-build/ Tip We recommend using the RelWithDebInfo build type by default for a good balance of debugging information and performance. The Debug , Release , and MinSizeRel build types are useful in more specific scenarios. In particular, note that several useful LLVM debugging features are only available in Debug builds. See the official CMake documentation for general details. What's next? \u00b6 Running tests \u00b6 Run all built tests through CTest : cd ../iree-build/ ctest --output-on-failure Take a look around \u00b6 Check out the contents of the 'tools' build directory: ls ../iree-build/iree/tools/ ../iree-build/iree/tools/iree-translate --help","title":"Getting started"},{"location":"building-from-source/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"building-from-source/getting-started/#prerequisites","text":"You will need to install CMake , the Ninja CMake generator, and the clang or MSVC C/C++ compilers: Note You are welcome to try different CMake generators and compilers, but IREE devs and CIs exclusively use these and other configurations are \"best effort\". Additionally, compilation on macOS is \"best effort\" as well, though we generally expect it to work due to its similarity with Linux. Patches to improve support for these are always welcome. Linux and macOS Install a compiler/linker (typically \"clang\" and \"lld\" package). Install CMake (typically \"cmake\" package). Install Ninja (typically \"ninja-build\" package). On a relatively recent Debian/Ubuntu: sudo apt install cmake ninja-build clang lld Windows Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Install CMake from the official downloads page Install Ninja either from the official site . Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details.","title":"Prerequisites"},{"location":"building-from-source/getting-started/#clone-and-build","text":"Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/google/iree.git cd iree git submodule update --init Configure then build all targets using CMake: Configure CMake: Linux and MacOS # Recommended for simple development using clang and lld: cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON \\ -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ \\ -DIREE_ENABLE_LLD = ON # Alternately, with system compiler and your choice of CMake generator: # cmake -B ../iree-build/ -S . # Additional quality of life CMake flags: # Enable ccache: # -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache Windows cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON Build: cmake --build ../iree-build/ Tip We recommend using the RelWithDebInfo build type by default for a good balance of debugging information and performance. The Debug , Release , and MinSizeRel build types are useful in more specific scenarios. In particular, note that several useful LLVM debugging features are only available in Debug builds. See the official CMake documentation for general details.","title":"Clone and build"},{"location":"building-from-source/getting-started/#whats-next","text":"","title":"What's next?"},{"location":"building-from-source/getting-started/#running-tests","text":"Run all built tests through CTest : cd ../iree-build/ ctest --output-on-failure","title":"Running tests"},{"location":"building-from-source/getting-started/#take-a-look-around","text":"Check out the contents of the 'tools' build directory: ls ../iree-build/iree/tools/ ../iree-build/iree/tools/iree-translate --help","title":"Take a look around"},{"location":"building-from-source/optional-features/","text":"Optional Features \u00b6 This page details the optional features and build modes for the project. Most of these are controlled by various CMake options, sometimes requiring extra setup or preparation. Each section extends the basic build steps in the getting started page. Building Python Bindings \u00b6 This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation >=3.7 (we aim to support non-eol Python versions ). Installation of python dependencies as specified in bindings/python/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option. Setup \u00b6 We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv iree.venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source iree.venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./bindings/python/build_requirements.txt Windows python -m venv . venv . venv \\ Scripts \\ activate . bat python -m pip install - -upgrade pip python -m pip install -r bindings \\ python \\ build_requirements . txt When done, close your shell or run deactivate . Usage \u00b6 From the iree-build directory: Linux and MacOS cmake \\ -GNinja \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_BUILD_PYTHON_BINDINGS = ON \\ -DPython3_EXECUTABLE = \" $( which python ) \" \\ . cmake --build . # Add ./bindings/python and compiler-api/python_package to PYTHONPATH and # use the API. source .env && export PYTHONPATH export PYTHONPATH = \" $PWD /bindings/python\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Windows cmake -GNinja -DCMAKE_BUILD_TYPE = RelWithDebInfo -DIREE_BUILD_PYTHON_BINDINGS = ON . cmake - -build . # Add bindings\\python and compiler-api\\python_package to PYTHONPATH and use # the API. set PYTHONPATH = \"$pwd\\compiler-api\\python_package;$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest. Building TensorFlow Frontend Bindings \u00b6 This section describes how to build compiler tools and Python bindings for importing models from various TensorFlow-ecosystem frontends, including TensorFlow, XLA (used for JAX), and TFLite. It extends the instructions in Building Python Bindings above with additional steps that are TensorFlow specific. There are various ways to achieve these ends, but this section describes the canonical method that the core developers recommend. Upon completing these steps, you will have access to additional Python packages: Python Import Description import iree.compiler.tools.tf Tools for importing from TensorFlow import iree.compiler.tools.tflite Tools for importing from TensorFlow Lite import iree.compiler.tools.xla Tools for importing from XLA These tools packages are needed in order for the frontend specific, high-level APIs under import iree.compiler.tf , import iree.compiler.tflite , import iree.compiler.xla , and import iree.jax to be fully functional. Setup \u00b6 A relatively recent tf-nightly release is needed to run tests. Linux and MacOS python -m pip install -r ./integrations/tensorflow/bindings/python/build_requirements.txt Windows python -m pip install -r integrations \\ tensorflow \\ bindings \\ python \\ build_requirements . txt TensorFlow \u00b6 TensorFlow frontends can only be built with Bazel , and this must be done as a manual step (we used to have automation for this, but Bazel integrates poorly with automation and it made diagnosis and cross platform usage unreliable). The recommended version of Bazel (used by CI systems) can be found in the .bazelversion file. In addition, Bazel is hard to use out of tree, so these steps will involve working from the source tree (instead of the build tree). Note Due to the difficulties using Bazel and compiling TensorFlow, only compilation on Linux with clang is supported. Other OS's and compilers are \"best effort\" with patches to improve support welcome. Linux and MacOS # From the iree source directory. cd integrations/tensorflow python ../../configure_bazel.py bazel build iree_tf_compiler:importer-binaries Windows # From the iree source directory. cd integrations \\ tensorflow python ..\\..\\ configure_bazel . py bazel build iree_tf_compiler : importer-binaries Importer binaries can be found under bazel-bin/iree_tf_compiler and can be used from the command line if desired. IREE \u00b6 The main IREE build will embed binaries built above and enable additional Python APIs. Within the build, the binaries are symlinked, so can be rebuilt per above without re-running these steps for edit-and-continue style work. # From the iree-build/ directory. cmake -DIREE_BUILD_TENSORFLOW_ALL = ON . cmake --build . # Validate. python -c \"import iree.tools.tf as _; print(_.get_tool('iree-import-tf'))\" python -c \"import iree.tools.tflite as _; print(_.get_tool('iree-import-tflite'))\" python -c \"import iree.tools.xla as _; print(_.get_tool('iree-import-xla'))\"","title":"Optional features"},{"location":"building-from-source/optional-features/#optional-features","text":"This page details the optional features and build modes for the project. Most of these are controlled by various CMake options, sometimes requiring extra setup or preparation. Each section extends the basic build steps in the getting started page.","title":"Optional Features"},{"location":"building-from-source/optional-features/#building-python-bindings","text":"This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation >=3.7 (we aim to support non-eol Python versions ). Installation of python dependencies as specified in bindings/python/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option.","title":"Building Python Bindings"},{"location":"building-from-source/optional-features/#setup","text":"We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv iree.venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source iree.venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./bindings/python/build_requirements.txt Windows python -m venv . venv . venv \\ Scripts \\ activate . bat python -m pip install - -upgrade pip python -m pip install -r bindings \\ python \\ build_requirements . txt When done, close your shell or run deactivate .","title":"Setup"},{"location":"building-from-source/optional-features/#usage","text":"From the iree-build directory: Linux and MacOS cmake \\ -GNinja \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_BUILD_PYTHON_BINDINGS = ON \\ -DPython3_EXECUTABLE = \" $( which python ) \" \\ . cmake --build . # Add ./bindings/python and compiler-api/python_package to PYTHONPATH and # use the API. source .env && export PYTHONPATH export PYTHONPATH = \" $PWD /bindings/python\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Windows cmake -GNinja -DCMAKE_BUILD_TYPE = RelWithDebInfo -DIREE_BUILD_PYTHON_BINDINGS = ON . cmake - -build . # Add bindings\\python and compiler-api\\python_package to PYTHONPATH and use # the API. set PYTHONPATH = \"$pwd\\compiler-api\\python_package;$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest.","title":"Usage"},{"location":"building-from-source/optional-features/#building-tensorflow-frontend-bindings","text":"This section describes how to build compiler tools and Python bindings for importing models from various TensorFlow-ecosystem frontends, including TensorFlow, XLA (used for JAX), and TFLite. It extends the instructions in Building Python Bindings above with additional steps that are TensorFlow specific. There are various ways to achieve these ends, but this section describes the canonical method that the core developers recommend. Upon completing these steps, you will have access to additional Python packages: Python Import Description import iree.compiler.tools.tf Tools for importing from TensorFlow import iree.compiler.tools.tflite Tools for importing from TensorFlow Lite import iree.compiler.tools.xla Tools for importing from XLA These tools packages are needed in order for the frontend specific, high-level APIs under import iree.compiler.tf , import iree.compiler.tflite , import iree.compiler.xla , and import iree.jax to be fully functional.","title":"Building TensorFlow Frontend Bindings"},{"location":"building-from-source/optional-features/#setup_1","text":"A relatively recent tf-nightly release is needed to run tests. Linux and MacOS python -m pip install -r ./integrations/tensorflow/bindings/python/build_requirements.txt Windows python -m pip install -r integrations \\ tensorflow \\ bindings \\ python \\ build_requirements . txt","title":"Setup"},{"location":"building-from-source/optional-features/#tensorflow","text":"TensorFlow frontends can only be built with Bazel , and this must be done as a manual step (we used to have automation for this, but Bazel integrates poorly with automation and it made diagnosis and cross platform usage unreliable). The recommended version of Bazel (used by CI systems) can be found in the .bazelversion file. In addition, Bazel is hard to use out of tree, so these steps will involve working from the source tree (instead of the build tree). Note Due to the difficulties using Bazel and compiling TensorFlow, only compilation on Linux with clang is supported. Other OS's and compilers are \"best effort\" with patches to improve support welcome. Linux and MacOS # From the iree source directory. cd integrations/tensorflow python ../../configure_bazel.py bazel build iree_tf_compiler:importer-binaries Windows # From the iree source directory. cd integrations \\ tensorflow python ..\\..\\ configure_bazel . py bazel build iree_tf_compiler : importer-binaries Importer binaries can be found under bazel-bin/iree_tf_compiler and can be used from the command line if desired.","title":"TensorFlow"},{"location":"building-from-source/optional-features/#iree","text":"The main IREE build will embed binaries built above and enable additional Python APIs. Within the build, the binaries are symlinked, so can be rebuilt per above without re-running these steps for edit-and-continue style work. # From the iree-build/ directory. cmake -DIREE_BUILD_TENSORFLOW_ALL = ON . cmake --build . # Validate. python -c \"import iree.tools.tf as _; print(_.get_tool('iree-import-tf'))\" python -c \"import iree.tools.tflite as _; print(_.get_tool('iree-import-tflite'))\" python -c \"import iree.tools.xla as _; print(_.get_tool('iree-import-xla'))\"","title":"IREE"},{"location":"building-from-source/riscv/","text":"RISC-V cross-compilation \u00b6 Running on a platform like RISC-V involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific RISC-V CPU architecture and operating system): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then pushed to the target to run natively. Prerequisites \u00b6 Host environment setup \u00b6 You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps. Install RISC-V cross-compile toolchain and emulator \u00b6 You'll need a RISC-V LLVM compilation toolchain and a RISC-V enabled QEMU emulator. See instructions in the following links Clang getting started RISC-V GNU toolchain QEMU RISC-V Linux QEMU Note The RISCV_TOOLCHAIN_ROOT environment variable needs to be set to the root directory of the installed GNU toolchain when building the RISC-V compiler target and the runtime library. Install prebuilt RISC-V tools (RISC-V 64-bit Linux toolchain) \u00b6 Execute the following script to download the prebuilt RISC-V toolchain and QEMU from the IREE root directory: ./build_tools/riscv/riscv_bootstrap.sh Support vector extension \u00b6 For RISC-V vector extensions support, see additional instructions Configure and build \u00b6 Host configuration \u00b6 Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install Target configuration \u00b6 The following instruction shows how to build for a RISC-V 64-bit Linux machine. For other RISC-V targets, please refer to riscv.toolchain.cmake as a reference of how to set up the cmake configuration. RISC-V 64-bit Linux target \u00b6 cmake -GNinja -B ../iree-build-riscv/ \\ -DCMAKE_TOOLCHAIN_FILE = \"./build_tools/cmake/riscv.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = $( realpath ../iree-build-host/install ) \\ -DRISCV_CPU = rv64 \\ -DIREE_BUILD_COMPILER = OFF \\ -DRISCV_TOOLCHAIN_ROOT = ${ RISCV_TOOLCHAIN_ROOT } \\ . Running IREE bytecode modules on the RISC-V system \u00b6 Note The following instructions are meant for the RISC-V 64-bit Linux target. For the bare-metal target, please refer to simple_embedding to see how to build a ML workload for a bare-metal machine. Set the path to qemu-riscv64 Linux emulator binary in the QEMU_BIN environment variable. If it is installed with riscv_bootstrap.sh , the path is default at ${HOME}/riscv/qemu/linux/RISCV/bin/qemu-riscv64. export QEMU_BIN = <path to qemu-riscv64 binary> Invoke the host compiler tools to produce a bytecode module flatbuffer: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmvx \\ iree/samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Run the RISC-V emulation: ${ QEMU_BIN } \\ -cpu rv64 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/iree/tools/iree-run-module \\ --driver = vmvx \\ --module_file = /tmp/simple_abs_vmvx.vmfb \\ --entry_function = abs \\ --function_input = f32 = -5 Optional configuration \u00b6 RISC-V Vector extensions allows SIMD code to run more efficiently. To enable the vector extension for the compiler toolchain and the emulator, build the tools from the following sources: RISC-V toolchain is built from https://github.com/llvm/llvm-project (main branch). Currently, the LLVM compiler is built on GNU toolchain, including libgcc, GNU linker, and C libraries. You need to build GNU toolchain first. Clone GNU toolchain from: https://github.com/riscv/riscv-gnu-toolchain (master branch). Switch the \"riscv-binutils\" submodule to rvv-1.0.x-zfh branch manually. RISC-V QEMU is built from https://github.com/sifive/qemu/tree/v5.2.0-rvv-rvb-zfh . The SIMD code can be generated following the IREE dynamic library CPU HAL driver flow with the additional command-line flags iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ -iree-llvm-target-triple = riscv64 \\ -iree-llvm-target-cpu = generic-rv64 \\ -iree-llvm-target-abi = lp64d \\ -iree-llvm-target-cpu-features = \"+m,+a,+f,+d,+experimental-v\" \\ -riscv-v-vector-bits-min = 256 -riscv-v-fixed-length-vector-lmul-max = 8 \\ iree_input.mlir -o mobilenet-dylib.vmfb Then run on the RISC-V QEMU: ${ QEMU_BIN } \\ -cpu rv64,x-v = true,x-k = true,vlen = 256 ,elen = 64 ,vext_spec = v1.0 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\"","title":"RISC-V cross-compilation"},{"location":"building-from-source/riscv/#risc-v-cross-compilation","text":"Running on a platform like RISC-V involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific RISC-V CPU architecture and operating system): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then pushed to the target to run natively.","title":"RISC-V cross-compilation"},{"location":"building-from-source/riscv/#prerequisites","text":"","title":"Prerequisites"},{"location":"building-from-source/riscv/#host-environment-setup","text":"You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.","title":"Host environment setup"},{"location":"building-from-source/riscv/#install-risc-v-cross-compile-toolchain-and-emulator","text":"You'll need a RISC-V LLVM compilation toolchain and a RISC-V enabled QEMU emulator. See instructions in the following links Clang getting started RISC-V GNU toolchain QEMU RISC-V Linux QEMU Note The RISCV_TOOLCHAIN_ROOT environment variable needs to be set to the root directory of the installed GNU toolchain when building the RISC-V compiler target and the runtime library.","title":"Install RISC-V cross-compile toolchain and emulator"},{"location":"building-from-source/riscv/#install-prebuilt-risc-v-tools-risc-v-64-bit-linux-toolchain","text":"Execute the following script to download the prebuilt RISC-V toolchain and QEMU from the IREE root directory: ./build_tools/riscv/riscv_bootstrap.sh","title":"Install prebuilt RISC-V tools (RISC-V 64-bit Linux toolchain)"},{"location":"building-from-source/riscv/#support-vector-extension","text":"For RISC-V vector extensions support, see additional instructions","title":"Support vector extension"},{"location":"building-from-source/riscv/#configure-and-build","text":"","title":"Configure and build"},{"location":"building-from-source/riscv/#host-configuration","text":"Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install","title":"Host configuration"},{"location":"building-from-source/riscv/#target-configuration","text":"The following instruction shows how to build for a RISC-V 64-bit Linux machine. For other RISC-V targets, please refer to riscv.toolchain.cmake as a reference of how to set up the cmake configuration.","title":"Target configuration"},{"location":"building-from-source/riscv/#risc-v-64-bit-linux-target","text":"cmake -GNinja -B ../iree-build-riscv/ \\ -DCMAKE_TOOLCHAIN_FILE = \"./build_tools/cmake/riscv.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = $( realpath ../iree-build-host/install ) \\ -DRISCV_CPU = rv64 \\ -DIREE_BUILD_COMPILER = OFF \\ -DRISCV_TOOLCHAIN_ROOT = ${ RISCV_TOOLCHAIN_ROOT } \\ .","title":"RISC-V 64-bit Linux target"},{"location":"building-from-source/riscv/#running-iree-bytecode-modules-on-the-risc-v-system","text":"Note The following instructions are meant for the RISC-V 64-bit Linux target. For the bare-metal target, please refer to simple_embedding to see how to build a ML workload for a bare-metal machine. Set the path to qemu-riscv64 Linux emulator binary in the QEMU_BIN environment variable. If it is installed with riscv_bootstrap.sh , the path is default at ${HOME}/riscv/qemu/linux/RISCV/bin/qemu-riscv64. export QEMU_BIN = <path to qemu-riscv64 binary> Invoke the host compiler tools to produce a bytecode module flatbuffer: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmvx \\ iree/samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Run the RISC-V emulation: ${ QEMU_BIN } \\ -cpu rv64 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/iree/tools/iree-run-module \\ --driver = vmvx \\ --module_file = /tmp/simple_abs_vmvx.vmfb \\ --entry_function = abs \\ --function_input = f32 = -5","title":"Running IREE bytecode modules on the RISC-V system"},{"location":"building-from-source/riscv/#optional-configuration","text":"RISC-V Vector extensions allows SIMD code to run more efficiently. To enable the vector extension for the compiler toolchain and the emulator, build the tools from the following sources: RISC-V toolchain is built from https://github.com/llvm/llvm-project (main branch). Currently, the LLVM compiler is built on GNU toolchain, including libgcc, GNU linker, and C libraries. You need to build GNU toolchain first. Clone GNU toolchain from: https://github.com/riscv/riscv-gnu-toolchain (master branch). Switch the \"riscv-binutils\" submodule to rvv-1.0.x-zfh branch manually. RISC-V QEMU is built from https://github.com/sifive/qemu/tree/v5.2.0-rvv-rvb-zfh . The SIMD code can be generated following the IREE dynamic library CPU HAL driver flow with the additional command-line flags iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ -iree-llvm-target-triple = riscv64 \\ -iree-llvm-target-cpu = generic-rv64 \\ -iree-llvm-target-abi = lp64d \\ -iree-llvm-target-cpu-features = \"+m,+a,+f,+d,+experimental-v\" \\ -riscv-v-vector-bits-min = 256 -riscv-v-fixed-length-vector-lmul-max = 8 \\ iree_input.mlir -o mobilenet-dylib.vmfb Then run on the RISC-V QEMU: ${ QEMU_BIN } \\ -cpu rv64,x-v = true,x-k = true,vlen = 256 ,elen = 64 ,vext_spec = v1.0 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\"","title":"Optional configuration"},{"location":"community/projects/","text":"Community projects \u00b6 The IREE Bare-Metal Arm Sample demonstrates how build IREE with the GNU Arm Embedded Toolchain for bare-metal Arm targets using the open-source firmware library libopencm3 . The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support and how to use a custom dialect alongside with the runtime. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Projects"},{"location":"community/projects/#community-projects","text":"The IREE Bare-Metal Arm Sample demonstrates how build IREE with the GNU Arm Embedded Toolchain for bare-metal Arm targets using the open-source firmware library libopencm3 . The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support and how to use a custom dialect alongside with the runtime. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Community projects"},{"location":"deployment-configurations/bare-metal/","text":"Run on a Bare-Metal Platform \u00b6 IREE supports CPU model execution on a bare-metal platform. That is, a platform without operating system support, and the executable is built with the machine-specific linker script and/or the board support package (BSP). Bare-metal deployment typically uses IREE's LLVM compiler target much like the CPU - Dylib configuration, but using a limited subset of IREE's CPU HAL driver at runtime to load and execute compiled programs. Prerequisites \u00b6 Out-of-tree bare-metal platform tools and source code for the system should be ready, such as Compilation toolchain Platform linker script Firmware libraries Please follow the instructions to retrieve the IREE compiler. Compile the model for bare-metal \u00b6 The model can be compiled with the following command from the IREE compiler build directory iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ -iree-llvm-link-embedded = true \\ -iree-llvm-target-triple = x86_64-pc-linux-elf \\ -iree-llvm-debug-symbols = false \\ iree/samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_dylib.vmfb In which iree-hal-target-backends=dylib-llvm-aot : Build the model for the dynamic library CPU HAL driver iree-llvm-link-embedded=true : Generate the dynamic library with LLD and the artifact can be loaded with the embedded library loader without invoking the dynamic library support iree-llvm-target-triple : Use the <arch>-pc-linux-elf LLVM target triple so the artifact has a fixed ABI to be rendered by the elf_module library iree-llvm-debug-symbols=false : To reduce the artifact size See generate.sh for example command-line instructions of some common architectures You can replace the MLIR file with the other MLIR model files, following the instructions Compiling the bare-metal model for static-library support \u00b6 See the static_library demo sample for an example and instructions on running a model with IREE's static_library_loader . By default, the demo targets the host machine when compiling. To produce a bare-metal compatible model, run iree-translate as in the previous example and add the additional -iree-llvm-static-library-output-path= flag to specify the static library destination. This will produce a .h\\.o file to link directly into the target application. Build bare-metal runtime from the source \u00b6 A few CMake options and macros should be set to build a subset of IREE runtime libraries compatible with the bare-metal platform. We assume there's no multi-thread control nor system library support in the bare-metal system. The model execution is in a single-thread synchronous fashion. Set CMake options \u00b6 set(IREE_BUILD_COMPILER OFF) : Build IREE runtime only set(CMAKE_SYSTEM_NAME Generic) : Tell CMake to skip targeting a specific operating system set(IREE_BINDINGS_TFLITE OFF) : Disable the TFLite binding support set(IREE_ENABLE_THREADING OFF) : Disable multi-thread library support set(IREE_HAL_DRIVERS_TO_BUILD \"Dylib_Sync;VMVX_Sync\") : Build only the dynamic library and VMVX runtime synchronous HAL drivers set(IREE_BUILD_TESTS OFF) : Disable tests until IREE supports running them on bare-metal platforms set(IREE_BUILD_SAMPLES ON) : Build simple_embedding example Todo Clean the list up after #6353 is fixed. Also, set the toolchain-specific cmake file to match the tool path, target architecture, target abi, linker script, system library path, etc. Define IREE macros \u00b6 -DIREE_PLATFORM_GENERIC : Let IREE to build the runtime library without targeting a specific platform -DIREE_SYNCHRONIZATION_DISABLE_UNSAFE=1 : Disable thread synchronization support -DIREE_FILE_IO_ENABLE=0 : Disable file I/O -DIREE_TIME_NOW_FN : A function to return the system time. For the bare-metal system, it can be set as -DIREE_TIME_NOW_FN=\\\"\\{ return 0;\\}\\\" as there's no asynchronous wait handling Examples of how to setup the CMakeLists.txt and .cmake file: IREE RISC-V toolchain cmake IREE Bare-Metal Arm Sample Bare-metal execution example \u00b6 See simple_embedding for generic platform to see how to use the IREE runtime library to build/run the IREE model for the bare-metal target.","title":"CPU - Bare-Metal"},{"location":"deployment-configurations/bare-metal/#run-on-a-bare-metal-platform","text":"IREE supports CPU model execution on a bare-metal platform. That is, a platform without operating system support, and the executable is built with the machine-specific linker script and/or the board support package (BSP). Bare-metal deployment typically uses IREE's LLVM compiler target much like the CPU - Dylib configuration, but using a limited subset of IREE's CPU HAL driver at runtime to load and execute compiled programs.","title":"Run on a Bare-Metal Platform"},{"location":"deployment-configurations/bare-metal/#prerequisites","text":"Out-of-tree bare-metal platform tools and source code for the system should be ready, such as Compilation toolchain Platform linker script Firmware libraries Please follow the instructions to retrieve the IREE compiler.","title":"Prerequisites"},{"location":"deployment-configurations/bare-metal/#compile-the-model-for-bare-metal","text":"The model can be compiled with the following command from the IREE compiler build directory iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ -iree-llvm-link-embedded = true \\ -iree-llvm-target-triple = x86_64-pc-linux-elf \\ -iree-llvm-debug-symbols = false \\ iree/samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_dylib.vmfb In which iree-hal-target-backends=dylib-llvm-aot : Build the model for the dynamic library CPU HAL driver iree-llvm-link-embedded=true : Generate the dynamic library with LLD and the artifact can be loaded with the embedded library loader without invoking the dynamic library support iree-llvm-target-triple : Use the <arch>-pc-linux-elf LLVM target triple so the artifact has a fixed ABI to be rendered by the elf_module library iree-llvm-debug-symbols=false : To reduce the artifact size See generate.sh for example command-line instructions of some common architectures You can replace the MLIR file with the other MLIR model files, following the instructions","title":"Compile the model for bare-metal"},{"location":"deployment-configurations/bare-metal/#compiling-the-bare-metal-model-for-static-library-support","text":"See the static_library demo sample for an example and instructions on running a model with IREE's static_library_loader . By default, the demo targets the host machine when compiling. To produce a bare-metal compatible model, run iree-translate as in the previous example and add the additional -iree-llvm-static-library-output-path= flag to specify the static library destination. This will produce a .h\\.o file to link directly into the target application.","title":"Compiling the bare-metal model for static-library support"},{"location":"deployment-configurations/bare-metal/#build-bare-metal-runtime-from-the-source","text":"A few CMake options and macros should be set to build a subset of IREE runtime libraries compatible with the bare-metal platform. We assume there's no multi-thread control nor system library support in the bare-metal system. The model execution is in a single-thread synchronous fashion.","title":"Build bare-metal runtime from the source"},{"location":"deployment-configurations/bare-metal/#set-cmake-options","text":"set(IREE_BUILD_COMPILER OFF) : Build IREE runtime only set(CMAKE_SYSTEM_NAME Generic) : Tell CMake to skip targeting a specific operating system set(IREE_BINDINGS_TFLITE OFF) : Disable the TFLite binding support set(IREE_ENABLE_THREADING OFF) : Disable multi-thread library support set(IREE_HAL_DRIVERS_TO_BUILD \"Dylib_Sync;VMVX_Sync\") : Build only the dynamic library and VMVX runtime synchronous HAL drivers set(IREE_BUILD_TESTS OFF) : Disable tests until IREE supports running them on bare-metal platforms set(IREE_BUILD_SAMPLES ON) : Build simple_embedding example Todo Clean the list up after #6353 is fixed. Also, set the toolchain-specific cmake file to match the tool path, target architecture, target abi, linker script, system library path, etc.","title":"Set CMake options"},{"location":"deployment-configurations/bare-metal/#define-iree-macros","text":"-DIREE_PLATFORM_GENERIC : Let IREE to build the runtime library without targeting a specific platform -DIREE_SYNCHRONIZATION_DISABLE_UNSAFE=1 : Disable thread synchronization support -DIREE_FILE_IO_ENABLE=0 : Disable file I/O -DIREE_TIME_NOW_FN : A function to return the system time. For the bare-metal system, it can be set as -DIREE_TIME_NOW_FN=\\\"\\{ return 0;\\}\\\" as there's no asynchronous wait handling Examples of how to setup the CMakeLists.txt and .cmake file: IREE RISC-V toolchain cmake IREE Bare-Metal Arm Sample","title":"Define IREE macros"},{"location":"deployment-configurations/bare-metal/#bare-metal-execution-example","text":"See simple_embedding for generic platform to see how to use the IREE runtime library to build/run the IREE model for the bare-metal target.","title":"Bare-metal execution example"},{"location":"deployment-configurations/cpu-dylib/","text":"Dynamic Library CPU HAL Driver \u00b6 IREE supports efficient model execution on CPU. IREE uses LLVM to compile dense computation in the model into highly optimized CPU native instruction streams, which are embedded in IREE's deployable format as dynamic libraries (dylibs). IREE uses its own low-overhead minimal dynamic library loader to load them and then schedule them with concrete workloads onto various CPU cores. Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc. Get runtime and compiler \u00b6 Get IREE runtime with dylib HAL driver \u00b6 You will need to get an IREE runtime that supports the dylib HAL driver so it can execute the model on CPU via dynamic libraries containing native CPU instructions. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib HAL driver is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DyLib to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target). Get compiler for CPU native instructions \u00b6 Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the LLVM-based dylib compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command. Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DYLIB-LLVM-AOT to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host). Compile and run the model \u00b6 With the compiler and runtime for dynamic libraries, we can now compile a model and run it on the CPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ iree_input.mlir -o mobilenet-dylib.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"CPU - Dylib"},{"location":"deployment-configurations/cpu-dylib/#dynamic-library-cpu-hal-driver","text":"IREE supports efficient model execution on CPU. IREE uses LLVM to compile dense computation in the model into highly optimized CPU native instruction streams, which are embedded in IREE's deployable format as dynamic libraries (dylibs). IREE uses its own low-overhead minimal dynamic library loader to load them and then schedule them with concrete workloads onto various CPU cores. Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc.","title":"Dynamic Library CPU HAL Driver"},{"location":"deployment-configurations/cpu-dylib/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"deployment-configurations/cpu-dylib/#get-iree-runtime-with-dylib-hal-driver","text":"You will need to get an IREE runtime that supports the dylib HAL driver so it can execute the model on CPU via dynamic libraries containing native CPU instructions.","title":"Get IREE runtime with dylib HAL driver"},{"location":"deployment-configurations/cpu-dylib/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib HAL driver is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DyLib to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target).","title":"Build runtime from source"},{"location":"deployment-configurations/cpu-dylib/#get-compiler-for-cpu-native-instructions","text":"","title":"Get compiler for CPU native instructions"},{"location":"deployment-configurations/cpu-dylib/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the LLVM-based dylib compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command.","title":"Download as Python package"},{"location":"deployment-configurations/cpu-dylib/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DYLIB-LLVM-AOT to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host).","title":"Build compiler from source"},{"location":"deployment-configurations/cpu-dylib/#compile-and-run-the-model","text":"With the compiler and runtime for dynamic libraries, we can now compile a model and run it on the CPU.","title":"Compile and run the model"},{"location":"deployment-configurations/cpu-dylib/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"deployment-configurations/cpu-dylib/#compile-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ iree_input.mlir -o mobilenet-dylib.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer.","title":"Compile using the command-line"},{"location":"deployment-configurations/cpu-dylib/#run-the-model","text":"","title":"Run the model"},{"location":"deployment-configurations/cpu-dylib/#run-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"deployment-configurations/gpu-vulkan/","text":"Vulkan GPU HAL Driver \u00b6 IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm. Todo Add IREE's GPU support matrix: what GPUs are supported; what GPUs are well optimized; etc. Prerequisites \u00b6 In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Linux Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Windows Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Get runtime and compiler \u00b6 Get IREE runtime with Vulkan HAL driver \u00b6 Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target). Get compiler for SPIR-V exchange format \u00b6 Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into. Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the SPIR-V compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command. Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan-SPIRV to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host). Compile and run the model \u00b6 With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vulkan-spirv \\ iree_input.mlir -o mobilenet-vulkan.vmfb Todo Choose the suitable target triple for the current GPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"GPU - Vulkan"},{"location":"deployment-configurations/gpu-vulkan/#vulkan-gpu-hal-driver","text":"IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm. Todo Add IREE's GPU support matrix: what GPUs are supported; what GPUs are well optimized; etc.","title":"Vulkan GPU HAL Driver"},{"location":"deployment-configurations/gpu-vulkan/#prerequisites","text":"In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Linux Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Windows Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.","title":"Prerequisites"},{"location":"deployment-configurations/gpu-vulkan/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"deployment-configurations/gpu-vulkan/#get-iree-runtime-with-vulkan-hal-driver","text":"Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan.","title":"Get IREE runtime with Vulkan HAL driver"},{"location":"deployment-configurations/gpu-vulkan/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target).","title":"Build runtime from source"},{"location":"deployment-configurations/gpu-vulkan/#get-compiler-for-spir-v-exchange-format","text":"Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into.","title":"Get compiler for SPIR-V exchange format"},{"location":"deployment-configurations/gpu-vulkan/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the SPIR-V compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command.","title":"Download as Python package"},{"location":"deployment-configurations/gpu-vulkan/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan-SPIRV to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host).","title":"Build compiler from source"},{"location":"deployment-configurations/gpu-vulkan/#compile-and-run-the-model","text":"With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU.","title":"Compile and run the model"},{"location":"deployment-configurations/gpu-vulkan/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"deployment-configurations/gpu-vulkan/#compile-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vulkan-spirv \\ iree_input.mlir -o mobilenet-vulkan.vmfb Todo Choose the suitable target triple for the current GPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer.","title":"Compile using the command-line"},{"location":"deployment-configurations/gpu-vulkan/#run-the-model","text":"","title":"Run the model"},{"location":"deployment-configurations/gpu-vulkan/#run-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"ml-frameworks/jax/","text":"JAX Integration \u00b6 Todo Issue#5454 : write this documentation","title":"JAX"},{"location":"ml-frameworks/jax/#jax-integration","text":"Todo Issue#5454 : write this documentation","title":"JAX Integration"},{"location":"ml-frameworks/tensorflow-lite/","text":"TensorFlow Lite Integration \u00b6 IREE supports compiling and running pre-trained TensorFlow Lite (TFLite) models. It converts a model to TOSA MLIR , then compiles it into a VM module. Prerequisites \u00b6 Download a pre-trained TFLite model from the list of hosted models , or use the TensorFlow Lite converter to convert a TensorFlow model to a .tflite flatbuffer. Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tflite-snapshot \\ -f https://github.com/google/iree/releases Warning The TensorFlow Lite package is currently only available on Linux and macOS. It is not available on Windows yet (see this issue ). Importing models \u00b6 Fist, import the TFLite model to TOSA MLIR: iree-import-tflite \\ sample.tflite \\ -o sample.mlir Next, compile the TOSA MLIR to a VM flatbuffer, using either the command line tools or the Python API : Using the command-line tool \u00b6 iree-translate \\ --iree-mlir-to-vm-bytecode-module \\ --iree-input-type = tosa \\ --iree-hal-target-backends = vmvx \\ sample.mlir \\ -o sample.vmfb Using the python API \u00b6 from iree.compiler import compile_str with open ( 'sample.mlir' ) as sample_tosa_mlir : compiled_flatbuffer = compile_str ( sample_tosa_mlir . read (), input_type = \"tosa\" , target_backends = [ \"vmvx\" ], extra_args = [ \"--iree-native-bindings-support=false\" , \"--iree-tflite-bindings-support\" ]) Todo Issue#5462 : Link to TensorFlow Lite bindings documentation once it has been written. The flatbuffer can then be loaded to a VM module and run through IREE's runtime. Samples \u00b6 Colab notebooks Text classification with TFLite and IREE An example smoke test of the TensorFlow Lite C API is available here . Todo Issue#3954 : Add documentation for an Android demo using the Java TFLite bindings , once it is complete at not-jenni/iree-android-tflite-demo .","title":"TensorFlow Lite"},{"location":"ml-frameworks/tensorflow-lite/#tensorflow-lite-integration","text":"IREE supports compiling and running pre-trained TensorFlow Lite (TFLite) models. It converts a model to TOSA MLIR , then compiles it into a VM module.","title":"TensorFlow Lite Integration"},{"location":"ml-frameworks/tensorflow-lite/#prerequisites","text":"Download a pre-trained TFLite model from the list of hosted models , or use the TensorFlow Lite converter to convert a TensorFlow model to a .tflite flatbuffer. Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tflite-snapshot \\ -f https://github.com/google/iree/releases Warning The TensorFlow Lite package is currently only available on Linux and macOS. It is not available on Windows yet (see this issue ).","title":"Prerequisites"},{"location":"ml-frameworks/tensorflow-lite/#importing-models","text":"Fist, import the TFLite model to TOSA MLIR: iree-import-tflite \\ sample.tflite \\ -o sample.mlir Next, compile the TOSA MLIR to a VM flatbuffer, using either the command line tools or the Python API :","title":"Importing models"},{"location":"ml-frameworks/tensorflow-lite/#using-the-command-line-tool","text":"iree-translate \\ --iree-mlir-to-vm-bytecode-module \\ --iree-input-type = tosa \\ --iree-hal-target-backends = vmvx \\ sample.mlir \\ -o sample.vmfb","title":"Using the command-line tool"},{"location":"ml-frameworks/tensorflow-lite/#using-the-python-api","text":"from iree.compiler import compile_str with open ( 'sample.mlir' ) as sample_tosa_mlir : compiled_flatbuffer = compile_str ( sample_tosa_mlir . read (), input_type = \"tosa\" , target_backends = [ \"vmvx\" ], extra_args = [ \"--iree-native-bindings-support=false\" , \"--iree-tflite-bindings-support\" ]) Todo Issue#5462 : Link to TensorFlow Lite bindings documentation once it has been written. The flatbuffer can then be loaded to a VM module and run through IREE's runtime.","title":"Using the python API"},{"location":"ml-frameworks/tensorflow-lite/#samples","text":"Colab notebooks Text classification with TFLite and IREE An example smoke test of the TensorFlow Lite C API is available here . Todo Issue#3954 : Add documentation for an Android demo using the Java TFLite bindings , once it is complete at not-jenni/iree-android-tflite-demo .","title":"Samples"},{"location":"ml-frameworks/tensorflow/","text":"TensorFlow Integration \u00b6 IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format . Prerequisites \u00b6 Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ -f https://github.com/google/iree/releases Warning The TensorFlow package is currently only available on Linux and macOS. It is not available on Windows yet (see this issue ). Importing models \u00b6 IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-import-tf command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers. From SavedModel on TensorFlow Hub \u00b6 IREE supports importing and using SavedModels from TensorFlow Hub . Using the command-line tool \u00b6 First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( loaded_model . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-import-tf . You can read the options supported via iree-import-tf -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-import-tf -tf-import-type = savedmodel_v1 \\ -tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-import-tf is installed as /path/to/python/site-packages/iree/tools/tf/iree-import-tf . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU . Training \u00b6 Todo Discuss training Samples \u00b6 Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference TensorFlow Hub Import End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory. Troubleshooting \u00b6 Missing serving signature in SavedModel \u00b6 Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"TensorFlow"},{"location":"ml-frameworks/tensorflow/#tensorflow-integration","text":"IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format .","title":"TensorFlow Integration"},{"location":"ml-frameworks/tensorflow/#prerequisites","text":"Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ -f https://github.com/google/iree/releases Warning The TensorFlow package is currently only available on Linux and macOS. It is not available on Windows yet (see this issue ).","title":"Prerequisites"},{"location":"ml-frameworks/tensorflow/#importing-models","text":"IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-import-tf command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers.","title":"Importing models"},{"location":"ml-frameworks/tensorflow/#from-savedmodel-on-tensorflow-hub","text":"IREE supports importing and using SavedModels from TensorFlow Hub .","title":"From SavedModel on TensorFlow Hub"},{"location":"ml-frameworks/tensorflow/#using-the-command-line-tool","text":"First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( loaded_model . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-import-tf . You can read the options supported via iree-import-tf -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-import-tf -tf-import-type = savedmodel_v1 \\ -tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-import-tf is installed as /path/to/python/site-packages/iree/tools/tf/iree-import-tf . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU .","title":"Using the command-line tool"},{"location":"ml-frameworks/tensorflow/#training","text":"Todo Discuss training","title":"Training"},{"location":"ml-frameworks/tensorflow/#samples","text":"Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference TensorFlow Hub Import End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory.","title":"Samples"},{"location":"ml-frameworks/tensorflow/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"ml-frameworks/tensorflow/#missing-serving-signature-in-savedmodel","text":"Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"Missing serving signature in SavedModel"}]}